{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Extracting text from a PDF file and splitting that text into chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-pymupdf4llm\n",
      "  Downloading langchain_pymupdf4llm-0.3.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.35 (from langchain-pymupdf4llm)\n",
      "  Downloading langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pymupdf4llm<0.1.0,>=0.0.17 (from langchain-pymupdf4llm)\n",
      "  Downloading pymupdf4llm-0.0.21-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Downloading langsmith-0.3.32-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (8.4.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (4.12.2)\n",
      "Collecting pydantic<3.0.0,>=2.5.2 (from langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pymupdf>=1.25.5 (from pymupdf4llm<0.1.0,>=0.0.17->langchain-pymupdf4llm)\n",
      "  Using cached pymupdf-1.25.5-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Downloading orjson-3.10.16-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (2.32.3)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Downloading httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (3.7)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arunekambaram/code/venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm) (2.2.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.35->langchain-pymupdf4llm)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain_pymupdf4llm-0.3.1-py3-none-any.whl (11 kB)\n",
      "Downloading langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
      "Downloading pymupdf4llm-0.0.21-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.3.32-py3-none-any.whl (358 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pymupdf-1.25.5-cp39-abi3-macosx_11_0_arm64.whl (18.6 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.10.16-cp312-cp312-macosx_15_0_arm64.whl (133 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.5/633.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, sniffio, pymupdf, pydantic-core, orjson, jsonpointer, h11, annotated-types, requests-toolbelt, pymupdf4llm, pydantic, jsonpatch, httpcore, anyio, httpx, langsmith, langchain-core, langchain-pymupdf4llm\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.54 langchain-pymupdf4llm-0.3.1 langsmith-0.3.32 orjson-3.10.16 pydantic-2.11.3 pydantic-core-2.33.1 pymupdf-1.25.5 pymupdf4llm-0.0.21 requests-toolbelt-1.0.0 sniffio-1.3.1 typing-inspection-0.4.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-pymupdf4llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file = \"/Users/arunekambaram/Desktop/RAG-ChatBot/data/GPT-4_VS_Human_translators (1).pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(uploaded_file)\n",
    "documents = loader.load()\n",
    "text = \"\\n\".join([doc.page_content for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PyMuPDF4LLMLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyMuPDF4LLMLoader\u001b[49m(\n\u001b[1;32m      2\u001b[0m    uploaded_file,\n\u001b[1;32m      3\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PyMuPDF4LLMLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# loader = PyMuPDF4LLMLoader(\n",
    "#    uploaded_file,\n",
    "#     mode=\"page\",\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# print(len(docs))\n",
    "# pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GPT-4 vs. Human Translators: A Comprehensive Evaluation of\\nTranslation Quality Across Languages, Domains, and Expertise Levels\\nJianhao Yan1,2∗\\nPingchuan Yan3∗\\nYulong Chen4∗\\nJudy Li5\\nXianchao Zhu5\\nYue Zhang2,6,\\x001 Zhejiang University\\n2 School of Engineering, Westlake University\\n3 University College London\\n4 University of Cambridge\\n5 Lan-Bridge Group\\n6 Institute of Advanced Technology, Westlake Institute for Advanced Study\\nelliottyan37@gmail.com\\nAbstract\\nThis study comprehensively evaluates the\\ntranslation quality of Large Language Mod-\\nels (LLMs), specifically GPT-4, against hu-\\nman translators of varying expertise lev-\\nels across multiple language pairs and do-\\nmains. Through carefully designed annota-\\ntion rounds, we find that GPT-4 performs\\ncomparably to junior translators in terms of'),\n",
       " Document(page_content='mains. Through carefully designed annota-\\ntion rounds, we find that GPT-4 performs\\ncomparably to junior translators in terms of\\ntotal errors made but lags behind medium\\nand senior translators. We also observe the\\nimbalanced performance across different lan-\\nguages and domains, with GPT-4’s transla-\\ntion capability gradually weakening from\\nresource-rich to resource-poor directions. In\\naddition, we qualitatively study the trans-\\nlation given by GPT-4 and human transla-\\ntors, and find that GPT-4 translator suffers\\nfrom literal translations, but human trans-\\nlators sometimes overthink the background\\ninformation. To our knowledge, this study\\nis the first to evaluate LLMs against human\\ntranslators and analyze the systematic differ-\\nences between their outputs, providing valu-'),\n",
       " Document(page_content='is the first to evaluate LLMs against human\\ntranslators and analyze the systematic differ-\\nences between their outputs, providing valu-\\nable insights into the current state of LLM-\\nbased translation and its potential limitations.\\n1\\nIntroduction\\nRecent studies show that LLMs can serve as a\\nstrong translation system and a good substitute for\\nNMT models (Jiao et al., 2023a; Wang et al., 2023a;\\nEnis and Hopkins, 2024; Huang et al., 2023; Wu\\net al., 2024a; Hendy et al., 2023; Peng et al., 2023).\\nFor example, Jiao et al. (2023a) and Wang et al.\\n(2023a) find that GPT-4 can outperform commer-\\ncial machine translation systems via automatic and\\nhuman evaluations. Such impressive results have\\nhastened a wide range of applications, including\\nthe use of GPT-4 for literary translation (Wu et al.,'),\n",
       " Document(page_content='hastened a wide range of applications, including\\nthe use of GPT-4 for literary translation (Wu et al.,\\n2024b).\\n∗These authors contributed equally to this work.\\nDespite their impressive capabilities, the nature\\nof LLM output compared to human translators\\nremains unclear.\\nThis raises two critical ques-\\ntions: (1) How do LLMs compare to human ex-\\nperts in translation quality? and (2) Are there\\nfundamental differences in their outputs? These\\ninquiries are particularly relevant in light of recent\\nresearch demonstrating significant distinctions be-\\ntween LLM-generated and human-generated texts\\nin general (Li et al., 2023; Bao et al., 2023). Such\\nfindings suggest that even if LLMs produce high-\\nquality translations, their outputs may possess\\nunique characteristics that distinguish them from'),\n",
       " Document(page_content='findings suggest that even if LLMs produce high-\\nquality translations, their outputs may possess\\nunique characteristics that distinguish them from\\nhuman-produced translations.\\nTo determine where LLMs fall within the spec-\\ntrum of human translation proficiency, which\\nranges from novice translators to seasoned profes-\\nsionals, we study the problem by taking the current\\nrepresentative LLM, i.e., GPT-4, and comparing\\nit against human translators with different exper-\\ntise. We first conduct a preliminary study compar-\\ning human translations against GPT-4 translations,\\nfinding that even experts cannot reach a consen-\\nsus on which translation is better. Given these\\nfindings, we take a finer-grained evaluation across\\ndifferent languages and domains, so that translation'),\n",
       " Document(page_content='findings, we take a finer-grained evaluation across\\ndifferent languages and domains, so that translation\\nquality can be better calibrated and systematic dif-\\nferences can be measured. Our evaluation covers\\nthree language pairs from resource-rich to resource-\\npoor, i.e., Chinese↔English, Russian↔English,\\nand Chinese↔Hindi, and three domains, i.e., News,\\nTechnology, and Biomedical. Given a source sen-\\ntence, we ask junior, medium, and senior trans-\\nlators and GPT-4 to generate the corresponding\\ntranslation in the target language. Then given each\\ntranslation pair, we hire independent expert annota-\\ntors to label the errors in the target sentence under\\nthe MQM schema (Freitag et al., 2021). We find\\nthat GPT-4 reaches a comparable performance to'),\n",
       " Document(page_content='tors to label the errors in the target sentence under\\nthe MQM schema (Freitag et al., 2021). We find\\nthat GPT-4 reaches a comparable performance to\\njunior translators in the perspective of total errors\\nmade, and lags behind senior ones with a consider-\\narXiv:2407.03658v1  [cs.CL]  4 Jul 2024'),\n",
       " Document(page_content='able gap.\\nOur further analyses and qualitative studies show\\nthat there are imbalanced performances for differ-\\nent languages and domains. From resource-rich to\\nresource-poor directions, GPT-4’s translation capa-\\nbility gradually weakens. For resource-rich direc-\\ntions like Chinese↔English, GPT-4 performs com-\\nparably with junior translators and even close to\\nmedium translators, but in Chinese↔Hindi, it even\\nlags behind our baseline system. The weaknesses\\nmentioned above are also general shortcomings of\\nlarge models and reflect that although large models\\nhave achieved universal translation with a focus\\non one language, translation between low-resource\\nlanguages remains a relative weakness.\\nTo our knowledge, we are the first to evaluate\\nLLMs against human translators and analyze the'),\n",
       " Document(page_content='languages remains a relative weakness.\\nTo our knowledge, we are the first to evaluate\\nLLMs against human translators and analyze the\\nsystematic differences between LLMs and human\\ntranslators.\\n2\\nRelated Work\\nBenchmarking LLMs\\nPrevious studies have\\nbenchmarked LLMs on various NLP tasks. Xu et al.\\n(2020) benchmark several LLMs on Chinese text,\\nevaluating their Chinese ability. Ye et al. (2024)\\nassess LLMs through Question Answering (QA),\\nMMLU (Hendrycks et al., 2021), and other metrics.\\nFrom these tests, LLMs with larger scales are gen-\\nerally proved to be more accurate except for certain\\ntasks. Yuan et al. (2023) demonstrates that LLMs\\nperform well in long-context understanding and\\nare more capable with Out-of-Distribution, which\\nmeans LLMs have a certain degree of generaliza-\\ntion ability.'),\n",
       " Document(page_content='are more capable with Out-of-Distribution, which\\nmeans LLMs have a certain degree of generaliza-\\ntion ability.\\nFurther to the MT field, Jiao et al. (2023b) find\\nthat GPT-4 performed competitively with other\\nSotA translation products. Wang et al. (2023a)\\nfurther investigated the capability of GPT-4 in\\ndocument-level translation, the results show that\\nGPT-4 performs better than commercial translation\\nproducts and document NMT methods. Compared\\nto them, our work empirically shows that GPT-4 is\\ncomparable to junior human translators.\\nLLMs as Human Experts\\nDue to the great ca-\\npacities of GPT-4 over traditional NLP models,\\nresearchers have investigated and compared the\\nperformance of GPT-4 as human experts in mul-\\ntiple NLP tasks. Zhu et al. (2024) highlight that'),\n",
       " Document(page_content='researchers have investigated and compared the\\nperformance of GPT-4 as human experts in mul-\\ntiple NLP tasks. Zhu et al. (2024) highlight that\\nGPT-4 and GPT-4-turbo show top performance on\\na Chinese financial language understanding task.\\nLiu et al. (2023b) find the LLMs can be benefi-\\ncial to biomedical NLP tasks. Goyal et al. (2022)\\ncompare GPT models with several summarization\\nmodels and humans, and find that GPT can gen-\\nerate summaries preferred by humans. In AI for\\neducation area, Nguyen and Allan (2024) show\\nGPT-4’s can provide teaching feedback for stu-\\ndents. Maloney et al. (2024) find that GPT-4 shows\\nclose performance compared with human partici-\\npants in coordination games. Siu (2023) show that\\nGPT-4 is comparable to humans on technical trans-'),\n",
       " Document(page_content='close performance compared with human partici-\\npants in coordination games. Siu (2023) show that\\nGPT-4 is comparable to humans on technical trans-\\nlation tasks. Bojic et al. (2023) find that GPT-4 can\\noutperform human experts on linguistic pragmatic\\ntasks. In clinical diagnostics, Han et al. (2023)\\nfind that GPT-4 can give comparable performance\\nto humans, and GPT-4v (vision version) can even\\noutperform human experts.\\nHuman Evaluation for MT\\n(Graham et al.,\\n2013) first propose Direct Assessment (DA), which\\nuses a continuous score from 0 to 100 to repre-\\nsent the quality of a hypothesis. DA has been\\nadopted in WMT translation tasks for the past\\nfew years (Farhad et al., 2021; Kocmi et al., 2022,\\n2023). MQM (Lommel et al., 2014), the annota-'),\n",
       " Document(page_content='adopted in WMT translation tasks for the past\\nfew years (Farhad et al., 2021; Kocmi et al., 2022,\\n2023). MQM (Lommel et al., 2014), the annota-\\ntion used in this paper, is another widely used an-\\nnotation scheme (Klubiˇcka et al., 2018; Rei et al.,\\n2020a). It requires the annotators to annotate the\\nerror span for each hypothesis and is shown to be\\nmore accurate and reliable than DA (Freitag et al.,\\n2021). Thus, it is utilized in the metrics tasks of\\n2022 and 2023 WMT challenges (Freitag et al.,\\n2022, 2023).\\nHuman Parity\\nThe human parity for machine\\ntranslation systems is first claimed by (Hassan et al.,\\n2018), which describes a comparable performance\\non the WMT 2017 news translation task from Chi-\\nnese to English when compared to professional hu-'),\n",
       " Document(page_content='2018), which describes a comparable performance\\non the WMT 2017 news translation task from Chi-\\nnese to English when compared to professional hu-\\nman translations. However, this claim is challenged\\nby the following research, raising concerns about\\nthe limited scope of human parity. These limita-\\ntions include the expertise of human evaluators (Fis-\\ncher and L¨\"aubli, 2020), the origin and quality of\\nsource sentences (Toral et al., 2018; Kim et al.,\\n2023), the limited scenario of comparison (Poibeau,\\n2022) and difficulty of translation (Graham et al.,\\n2020), indicating significant gaps between NMT\\nmodels and the professional translators. In this\\nwork, we evaluate whether the SOTA LLM GPT-\\n4 performs comparable to professional translators\\nand what differs between human translators and'),\n",
       " Document(page_content='work, we evaluate whether the SOTA LLM GPT-\\n4 performs comparable to professional translators\\nand what differs between human translators and\\nLLMs. With the above lessons in mind, we address'),\n",
       " Document(page_content='these limitations by hiring expert annotators, avoid-\\ning target-origin source text, manually evaluating\\nsource sentences, and covering high-resource to\\nlow-resource language pairs and various domains.\\n3\\nPreliminary Study\\nThis section presents our preliminary study. We\\naim to first compare GPT-4 translations with hu-\\nman translations qualitatively, in a coarse manner.\\nOur comparison is simple and direct. We sample\\nhuman-translated texts and prompt GPT-4 to trans-\\nlate the same source sentence. Then, we ask expert\\nannotators to determine which translation is better.\\nParticularly, to have a quick overview of the qual-\\nities of human translations against GPT-4 transla-\\ntions, we first utilize COMET-QE1 to score our\\nin-house Chinese to English human-translated doc-'),\n",
       " Document(page_content='ities of human translations against GPT-4 transla-\\ntions, we first utilize COMET-QE1 to score our\\nin-house Chinese to English human-translated doc-\\numents, and select two documents with the highest\\nscore and the lowest score. Note that our in-house\\ntranslated documents are all translated by profes-\\nsional translators. In this way, we gather 40 pairs\\nof translations from professional translators and\\nGPT-4, respectively. Recent findings (Freitag et al.,\\n2021) have demonstrated that crowd-sourced hu-\\nman ratings are less reliable for high-quality MT\\nevaluation. Thus, we hire six expert annotators to\\ncompare the two translations and select the better\\ntranslations they find. We randomly shuffle the\\nGPT-4 and human translations to prevent annota-\\ntors from identifying GPT-4.\\nThe\\naverage\\nwin'),\n",
       " Document(page_content='translations they find. We randomly shuffle the\\nGPT-4 and human translations to prevent annota-\\ntors from identifying GPT-4.\\nThe\\naverage\\nwin\\nrate\\nof\\nGPT\\nis\\n15.5/40 (36.25%).\\nIt looks like a clear win\\nfor human translators, but when delving deeper,\\nwe find that the expert annotators have a low\\nratio of agreement with each other. In Table 1,\\nmost annotators only agree with each other at\\naround 60% (the baseline is 50%) of an agreed\\nwinner at each source sentence.\\nWe further\\nconduct a significance test and only annotator\\nB finds human translation significantly better\\nthan GPT’s translation and other annotators\\nhave high p-values. Given annotators’ expertise\\nand our task is straightforward, these results\\nindicate that even expert annotators find it difficult'),\n",
       " Document(page_content='have high p-values. Given annotators’ expertise\\nand our task is straightforward, these results\\nindicate that even expert annotators find it difficult\\nto agree on which translation is better, and\\nGPT-generated translations might have different\\nadvantages against human-generated ones. These\\nresults motivate us to conduct a finer-grained and\\ncomprehensive evaluation to reveal the systematic\\ndifference between GPT-4 and human translations.\\n1Unbabel/wmt23-cometkiwi-da-xl\\nAnnotators\\nA\\nB\\nC\\nD\\nE\\nF\\nA\\n100.0\\n57.5\\n65.0\\n65.0\\n62.5\\n67.5\\nB\\n-\\n100.0\\n52.5\\n52.5\\n50.0\\n50.0\\nC\\n-\\n-\\n100.0\\n65.0\\n82.5\\n67.5\\nD\\n-\\n-\\n-\\n100.0\\n57.5\\n62.5\\nE\\n-\\n-\\n-\\n-\\n100.0\\n70.0\\nF\\n-\\n-\\n-\\n-\\n-\\n100.0\\np-value\\n1.000\\n0.038\\n0.268\\n0.081\\n0.154\\n0.875\\nTable 1: Ratio(%) of agreed winner across expert\\nannotators and significance p-value for binomial'),\n",
       " Document(page_content='-\\n100.0\\np-value\\n1.000\\n0.038\\n0.268\\n0.081\\n0.154\\n0.875\\nTable 1: Ratio(%) of agreed winner across expert\\nannotators and significance p-value for binomial\\ntest. P-value < 0.05 denotes a significant difference\\nbetween GPT-4 and Human.\\n4\\nMain Experimental Setup\\nMotivated by the results from our preliminary\\nstudy, we conduct a comprehensive and fine-\\ngrained evaluation, for revealing the systematic\\ndifference between humans and GPTs. Specifically,\\nwe employed the widely recognized Multidimen-\\nsional Quality Metrics (MQM) framework (Lom-\\nmel et al., 2014) and compared human translators\\nwith varying levels of expertise to GPT-4. Our\\nevaluation spans multiple languages and domains,\\naiming to furnish broad insights into these compar-\\nisons.\\n4.1\\nData Collection'),\n",
       " Document(page_content='evaluation spans multiple languages and domains,\\naiming to furnish broad insights into these compar-\\nisons.\\n4.1\\nData Collection\\nWe collect multilingual and multi-domain source\\nsentences. Our multilingual evaluation data con-\\ntains six language directions, covering high re-\\nsource to low resource, including English to Chi-\\nnese, Chinese to English, English to Russian, Rus-\\nsian to English, English to Hindi, and Hindi to\\nEnglish.\\nFor general domain Chinese⇔English and\\nEnglish⇔Russian, we sample source sentences\\nfrom test sets of WMT2023 and WMT2022, re-\\nspectively. For Chinese⇔Hindi, we extract source\\nnews text from public websites. For multi-domain\\nevaluation data, we evaluate two domains, i.e.,\\nbiomedical and technology and we evaluate Chi-'),\n",
       " Document(page_content='news text from public websites. For multi-domain\\nevaluation data, we evaluate two domains, i.e.,\\nbiomedical and technology and we evaluate Chi-\\nnese to English. The source sentences are extracted\\nnews texts from public websites. We ensure that\\nall sources are source language origin to avoid the\\neffect of translationese. We manually evaluate all\\nsource sentences for these tasks and ensure the\\nsource sentences are not too easy or too short. Fi-\\nnally, each task contains 200 sentences, making our\\nevaluation a total of 1600 sentences.'),\n",
       " Document(page_content='Type\\nError Name\\nExplanations\\nAccuracy\\nMistranslation\\nTranslation does not accurately represent the source.\\nAddition\\nInformation not present in the source.\\nMT Hallucination\\nInformation that has nothing related to source; or gibber-\\nish; or repeats\\nOmission\\nMissing content from the source.\\nUntranslated\\nNot translated.\\nWrong Name Entity and Term\\nWrong usage of NE and Terminology.\\nFluency\\nGrammar\\nProblems with grammar of target language.\\nPunctuation\\nIncorrect punctuation (for locale or style)\\nSpelling\\nIncorrect spelling or capitalization.\\nRegister\\nWrong grammatical register (e.g., inappropriately infor-\\nmal pronouns).\\nInconsistent Style\\nInternal inconsistency ( not related to terminology )\\nUnnatural Flow\\nTranslations that are too literal or sound unnatural.\\nOther\\nNon-translation\\n-'),\n",
       " Document(page_content='Internal inconsistency ( not related to terminology )\\nUnnatural Flow\\nTranslations that are too literal or sound unnatural.\\nOther\\nNon-translation\\n-\\nTable 2: Error category and explanations. We mainly follow the guidelines from Unbabel, and merge some\\nerrors to reduce the efforts for annotators to understand the annotation system. Concrete examples for\\neach error category can be found in the Appendix.\\n4.2\\nHuman Translators and Machine\\nTranslators\\nWe ask different human translators to translate our\\nsource sentences into the target language. Transla-\\ntors are of three different levels of expertise, cate-\\ngorized into junior-level, medium-level, and senior-\\nlevel translators. The level of expertise is ranked\\nby in-house criteria covering the translators’ edu-'),\n",
       " Document(page_content='level translators. The level of expertise is ranked\\nby in-house criteria covering the translators’ edu-\\ncational background, translation experience, and\\npractical proficiency. See Appendix A for more de-\\ntails. For a fair comparison, we request the experts\\nnot use machine translation or GPTs as assistance.\\nFor all directions except Zh-Hi and Hi-Zh, we col-\\nlect three human translation results from each level\\nof expertise. For Zh-Hi and Hi-Zh, we only have\\nmedium-level and senior-level translators due to\\nthe scarcity of translators.\\nExcept\\nfor\\nhuman\\ntranslators,\\nwe\\nuse\\ngpt-4-1106-preview,\\nthe\\ncurrent\\nstate-\\nof-the-art large language model released by\\nOpenAI and Seamless M4T (Communication\\net al., 2023) as the representative of traditional ma-'),\n",
       " Document(page_content='state-\\nof-the-art large language model released by\\nOpenAI and Seamless M4T (Communication\\net al., 2023) as the representative of traditional ma-\\nchine translations to complement our experiments.\\nWe directly prompt GPT-4 to obtain the translation,\\nas it is the most common practice for normal users,\\nthe easiest to reproduce, and to avoid confusion by\\nvarious techniques.\\n4.3\\nPrompt Search\\nPrevious study (Zhao et al., 2021; Liu et al., 2023a)\\nshows that different prompts with LLMs can result\\nin distinctive performance. Thus, we collect three\\ncandidate prompts used in previous research (Xu\\net al., 2023; Jiao et al., 2023a) and use COMET-\\nQE (Rei et al., 2020b) to select the best prompt\\nto make the best use of GPT-4, as shown in Table\\n3. In particular, we use these three prompts to'),\n",
       " Document(page_content='QE (Rei et al., 2020b) to select the best prompt\\nto make the best use of GPT-4, as shown in Table\\n3. In particular, we use these three prompts to\\nprompt GPT-4 to translate 100 source sentences in\\nour Chinese-to-English test set and adopt COMET-\\nQE to evaluate the quality of translations. We find\\nthat the third prompt yields the best performance,\\nand hence we adopt this prompt for all following\\nexperiments.\\n4.4\\nAnnotation Protocol\\nTo evaluate the results of candidates’ systems,\\nwe hire experts to annotate the errors of trans-\\nlations blindly. The annotation platform is Doc-\\ncano (Nakayama et al., 2018), and the error tags\\nare made according to MQM standards. MQM\\nrequires the annotators to annotate the span of er-\\nrors in each hypothesis. All hypotheses of the same'),\n",
       " Document(page_content='are made according to MQM standards. MQM\\nrequires the annotators to annotate the span of er-\\nrors in each hypothesis. All hypotheses of the same\\nsource sentence are shown to the annotator together\\nto help decide which is better. We have 13 error\\ncategories and two severities, as shown in Table\\n2. Our categorization for errors mostly follows'),\n",
       " Document(page_content='Prompt\\nCOMET\\nPlease translate the following sen-\\ntence from Chinese into English.\\nYour language and style should align\\nwith the language conventions of a\\nnative speaker. \\\\n{SOURCE}\\\\n\\n0.775\\nYou are an expert translator for trans-\\nlating Chinese to English.\\nYour\\nlanguage and style should align\\nwith the language conventions of\\na native speaker.\\n\\\\n[Chinese]:\\n{SOURCE}\\\\n[English]:\\n0.755\\nPlease provide the English transla-\\ntion for these sentences. Your lan-\\nguage and style should align with\\nthe language conventions of a native\\nspeaker. \\\\n{SOURCE}\\\\n\\n0.780\\nTable 3: Taking Chinese to English as an example,\\nour three prompts and corresponding scores with\\nCOMET-QE. {SOURCE} represents the source sen-\\ntence to be translated.\\nUnbabel’s practice 2 and we focus on most com-'),\n",
       " Document(page_content='COMET-QE. {SOURCE} represents the source sen-\\ntence to be translated.\\nUnbabel’s practice 2 and we focus on most com-\\nmon error types. Each tag has subtags with two\\nseverities, i.e., Minor or Major. A screenshot of the\\nannotation system is given in Figure 5.\\nFor each task, we first ask the two expert anno-\\ntators to carefully read our manual and conduct\\na training round on the first 10 groups of transla-\\ntions. Then, we manually check these annotations\\nto provide feedback and ask the two annotators to\\ncheck their disagreements and revise their results.\\nAfter two rounds of such training processes, we\\nask the annotators to finish the remaining sentences\\nwithout knowing each other’s results.\\nAfter the first round of annotation, we conduct a'),\n",
       " Document(page_content='ask the annotators to finish the remaining sentences\\nwithout knowing each other’s results.\\nAfter the first round of annotation, we conduct a\\nsecond round to further refine the evaluation results.\\nIn particular, we hire another two experts for each\\ntask and show them the previous annotation results.\\nThey are asked to approve and make necessary\\nmodifications to previous round annotations.\\n4.5\\nInter-Annotator Agreement\\nError annotation with MQM is challenging, and\\nprevious work demonstrates that the agreement\\nscores between MQM annotations are relatively\\nlow (Lommel et al., 2014). Reasons for this could\\nbe disagreement on precise spans and ambiguous\\n2https://help.unbabel.\\ncom/hc/en-us/articles/\\n6444304419479-Annotation-Guidelines-Typology-3-0\\nTask\\nCohen Kappa(Segment)\\nKrippendorffs(Span)'),\n",
       " Document(page_content='2https://help.unbabel.\\ncom/hc/en-us/articles/\\n6444304419479-Annotation-Guidelines-Typology-3-0\\nTask\\nCohen Kappa(Segment)\\nKrippendorffs(Span)\\nReference, Re-Annotated by (Freitag et al., 2021)\\nWMT 2020 En-De\\n0.208\\n0.456\\nWMT 2021 En-De\\n0.230\\n0.501\\nOurs\\nGeneral Zh-En\\n0.257\\n0.436\\nGeneral En-Zh\\n0.544\\n0.579\\nGeneral En-Ru\\n0.461\\n0.566\\nGeneral Ru-En\\n0.341\\n0.875\\nGeneral Zh-Hi\\n0.256\\n0.443\\nGeneral Hi-Zh\\n0.234\\n0.495\\nTechnology Zh-En\\n0.306\\n0.581\\nBiomedical Zh-En\\n0.373\\n0.616\\nAverage\\n0.321\\n0.555\\nTable 4: Cohen Kappa (segment-level) and Krip-\\npendorffs’ Alpha (span-level) agreement of our an-\\nnotations.\\nerror categorization (Lommel et al., 2014). Despite\\nthe low agreement scores, MQM is more reliable\\nthan other evaluation protocols like Direct Assess-\\nment (Freitag et al., 2021).'),\n",
       " Document(page_content='the low agreement scores, MQM is more reliable\\nthan other evaluation protocols like Direct Assess-\\nment (Freitag et al., 2021).\\nTo compute inter-annotator agreement for MQM,\\nwe employ segment-level Cohen’s Kappa (Cohen,\\n1960) and span-level Krippendorff’s alpha (Krip-\\npendorff, 1980). For reference, we calculate the\\nagreement on the annotated results of the 2020 and\\n2021 WMT English-to-German tasks by (Freitag\\net al., 2021). Our IAA results are shown in Table 4.\\nThanks to our two-round annotation process, our\\nIAA scores show a favorable agreement, indicating\\na good annotation quality.\\n5\\nMain Results\\n5.1\\nOverall Results\\nAnalysis of Error Severity\\nThe upper part of\\nFigure 1 plots the averaged number of errors of\\ndifferent systems and translators. Compared to our'),\n",
       " Document(page_content='Analysis of Error Severity\\nThe upper part of\\nFigure 1 plots the averaged number of errors of\\ndifferent systems and translators. Compared to our\\nMT baseline (seamless), GPT-4 has much fewer er-\\nrors. It performs almost as well as the junior-level\\ntranslator at the level of total errors, as GPT-4 is\\nannotated with only slightly more minor and major\\nerrors than junior translators. However, GPT-4 still\\nhas clear performance gaps between medium or\\nsenior human translators, as it makes considerably\\nmore mistakes than experienced translators. To our\\nknowledge, we are the first to report how GPT-4 is\\non translation against human translators.\\nAnalysis of Error Categories\\nFurthermore, we\\nplot the categories of errors in the bottom part of\\nFigure 1. Compared with junior human translators,'),\n",
       " Document(page_content='Analysis of Error Categories\\nFurthermore, we\\nplot the categories of errors in the bottom part of\\nFigure 1. Compared with junior human translators,\\nGPT-4 makes more errors in the accuracy of trans-'),\n",
       " Document(page_content='seamless\\ngpt4\\njunior\\nmedium\\nsenior\\nSystems\\n0\\n50\\n100\\n150\\n200\\nNumber of Errors\\nAverage Severity for Each System\\nMinor\\nMajor\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\nSystems\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200\\nNumber of Errors\\nError Category for Each System\\nAccuracy\\nFluency\\nFigure 1: Upper: Error severity for each system.\\nThe gray line represents the standard deviation for\\neach system across tasks. Bottom: Error category\\nanalysis for each system.\\nlations, which accounts for most of the disparity.\\nInterestingly, GPT-4 surpasses junior translators\\nin fluency issues, denoting a better capability of\\nlanguage usage.\\nIn addition, Figure 2 shows the top 5 categories\\nof errors made by different systems. ‘Mistransla-\\ntion’ is the most frequent error made by all systems.'),\n",
       " Document(page_content='of errors made by different systems. ‘Mistransla-\\ntion’ is the most frequent error made by all systems.\\nImproving much over the seamless baseline, GPT-\\n4 makes comparable numbers of ‘Mistranslation’\\nwith junior and medium human translators.\\nFor all translators, ‘Unnatural Flow’ is among\\nthe most frequent errors. Seamless, GPT-4, and\\njunior translators have similar levels of ‘Unnatural\\nFlow’, indicating possible issues of literal transla-\\ntion and not following language conventions. In\\ncontrast, medium and senior translators are anno-\\ntated with significantly fewer errors of ‘Unnatural\\nFlow’.\\nIn addition, we notice even though GPT-4 makes\\nMistranslation\\nPunctuation\\nWrong NE\\nUnnatural Flow\\nOmission\\nMistranslation\\nUnnatural Flow\\nWrong NE\\nGrammar\\nPunctuation\\nMistranslation\\nUnnatural Flow'),\n",
       " Document(page_content='Mistranslation\\nPunctuation\\nWrong NE\\nUnnatural Flow\\nOmission\\nMistranslation\\nUnnatural Flow\\nWrong NE\\nGrammar\\nPunctuation\\nMistranslation\\nUnnatural Flow\\nGrammar\\nWrong NE\\nPunctuation\\nMistranslation\\nUnnatural Flow\\nWrong NE\\nOmission\\nGrammar\\nMistranslation\\nUnnatural Flow\\nAddition\\nPunctuation\\nGrammar\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nNumber of Errors\\nTop5 Error Categories\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\nFigure 2: Top 5 categories of errors made by each\\nsystem.\\nmuch fewer ‘Wrong Name Entity(NE)’ errors com-\\npared to Seamless, which could be beneficial be-\\ncause of its huge knowledge acquired in the pre-\\ntraining stage, it still has a gap compared to human\\ntranslators.\\nFinally, we notice that GPT-4 does not have\\nOmission or Addition problems in its top-5 errors,'),\n",
       " Document(page_content='translators.\\nFinally, we notice that GPT-4 does not have\\nOmission or Addition problems in its top-5 errors,\\nwhereas even senior translators have Addition er-\\nrors.\\n5.2\\nDetailed Results for Each Language\\nIn Figure 3, we present detailed results for each\\nlanguage pair, averaged over two directions.\\nEnglish-Chinese\\nFrom Figure 3(a),\\nGPT-4\\nshows the great capability of translating English\\nto Chinese and vice versa. From the radar chart,\\nwe can see that GPT-4 makes almost the same or\\nslightly fewer semantic errors (Omission, Addition,\\nand Mistranslation errors) than Junior and Senior\\ntranslators. Especially mistranslation errors, which\\nare generally considered most semantically detri-\\nmental, are better than junior and senior translators.\\nFor omission and addition errors, GPT-4 reaches al-'),\n",
       " Document(page_content='mental, are better than junior and senior translators.\\nFor omission and addition errors, GPT-4 reaches al-\\nmost the same level as senior translators. However,\\nGPT-4 made significantly more lexical, stylistic,\\nand grammatical errors than human translators do.\\nThe error distribution of translation of GPT-4 meets\\nour expectations, as in the absence of reference,\\nGPT-4 will translate unfamiliar words directly and\\nliterally instead of seeking online materials or other\\nforms of help like human translators. Furthermore,\\ndue to the complexity and variability of Chinese,\\nthe translation of entity names or proper nouns is\\nusually not one-to-one, two above reasons together'),\n",
       " Document(page_content='Mistranslation(130)\\nAddition(4)\\nMT Hallucination(14)\\nOmission(29)\\nUntranslated(0)\\nWrong Name Entity & Term(78)\\nGrammar(9)\\nPunctuation(11)\\nSpelling(4)\\nRegister(8)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(a) Chinese↔English\\nMistranslation(20)\\nAddition(8)\\nMT Hallucination(0)\\nOmission(17)\\nUntranslated(2)\\nWrong Name Entity & Term(1)\\nGrammar(5)\\nPunctuation(3)\\nSpelling(3)\\nRegister(0)\\nInconsistent Style(3)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(b) English↔Russian\\nMistranslation(52)\\nAddition(6)\\nMT Hallucination(5)\\nOmission(9)\\nUntranslated(6)\\nWrong Name Entity & Term(7)\\nGrammar(6)\\nPunctuation(10)\\nSpelling(3)\\nRegister(1)\\nInconsistent Style(1)\\nNon-translation(0)'),\n",
       " Document(page_content='Omission(9)\\nUntranslated(6)\\nWrong Name Entity & Term(7)\\nGrammar(6)\\nPunctuation(10)\\nSpelling(3)\\nRegister(1)\\nInconsistent Style(1)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nseamless\\ngpt4\\nmedium\\nsenior\\n(c) Chinese↔Hindi\\nFigure 3: Error category results for each language. Each sub-figure is the average over two directions.\\nWe only include ‘Major’ errors here to highlight the most severe problems. Higher values indicate more\\nerrors and the number after each error type is the maximum number of that error.\\ncause the inferiority of the performance of GPT-4\\nin these aspects.\\nEnglish-Russian\\nFor the English-Russian trans-\\nlation tasks, GPT-4 made slightly more semantic\\nerrors but the number of mistranslation errors made\\nby GPT-4 is almost at the same level as medium'),\n",
       " Document(page_content='lation tasks, GPT-4 made slightly more semantic\\nerrors but the number of mistranslation errors made\\nby GPT-4 is almost at the same level as medium\\nand senior translators. However, GPT-4 generally\\nmade less stylistic, grammatical, and wrong name\\nentity & term than junior translators. The English-\\nRussian translation tasks are quite challenging and\\nthe performance of translators varies significantly,\\nbut GPT-4 still maintains the average level overall.\\nHindi-Chinese\\nAs the low-resource language\\npair we evaluate, GPT-4 demonstrates the worst\\nperformance across evaluated translators. We ob-\\nserve that GPT4 is inferior to our MT baseline. This\\nmay be due to the small portion of Hindi and Chi-\\nnese corpora in its pre-training dataset. Specifically,'),\n",
       " Document(page_content='may be due to the small portion of Hindi and Chi-\\nnese corpora in its pre-training dataset. Specifically,\\nmaking the most ‘Mistranslation’ errors of GPT-4\\nindicates a distance away from the language under-\\nstanding of human translators. As a comparison,\\nSeamlessM4T performs better in both semantic and\\nlexical errors.\\nDiscussion\\nOur results here manifest an imbal-\\nance of multilinguality for LLMs (Wang et al.,\\n2023b). Our results imply that GPT-4 can serve\\nas a reliable translator for resource-high such as\\nChinese to English but is doubtful for low-resource\\ndirections like Chinese-Hindi. In the low-resource\\nscenario, machine translator is more reliable.\\n5.3\\nDetailed Results for Different Domains\\nFigure 4 presents our results for different domains'),\n",
       " Document(page_content='scenario, machine translator is more reliable.\\n5.3\\nDetailed Results for Different Domains\\nFigure 4 presents our results for different domains\\nin Chinese-to-English translation. We compare\\nthree different domains, including news, technol-\\nogy, and biomedical.\\nGeneral News Domain\\nGPT-4 performs worse\\nin the news domain than human translators of three\\nlevels. The number of semantic errors made by\\nGPT-4 is quite close to junior and medium transla-\\ntors. Nonetheless, GPT-4 made more lexical and\\ngrammatical errors compared to human translators.\\nWe hypothesize the reasons for the situation de-\\nscribed above to happen are mainly because of the\\nliterariness and timeliness. Because GPT-4 is not\\nable to access the online materials to confirm the\\nname of a specific entity or event.\\nTechnology Domain'),\n",
       " Document(page_content='able to access the online materials to confirm the\\nname of a specific entity or event.\\nTechnology Domain\\nThe performance of GPT-4\\nis relatively close to medium-level translators. Ex-\\ncept for the Wrong Name Entity & Terms, GPT-4\\nmakes almost the same or even fewer errors than\\nmedium-level translators across all aspects. Specif-\\nically, the number of semantic errors made by GPT-\\n4 is almost the same to medium-level translators\\nand it makes much fewer structural and grammati-\\ncal errors. It means that in this field, GPT-4 might\\nunderstand the original text better than junior or\\nmedium-level translators and be able to conduct\\na translation that is more in line with the original\\nmeaning.\\nBiomedical Domain\\nSimilar to the technology\\ndomain, the qualities of the translations made by'),\n",
       " Document(page_content='meaning.\\nBiomedical Domain\\nSimilar to the technology\\ndomain, the qualities of the translations made by\\nGPT-4 and medium-level translators stand at the'),\n",
       " Document(page_content='Mistranslation(105)\\nAddition(4)\\nMT Hallucination(2)\\nOmission(27)\\nUntranslated(0)\\nWrong Name Entity & Term(75)\\nGrammar(9)\\nPunctuation(7)\\nSpelling(2)\\nRegister(6)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(a) General news domain.\\nMistranslation(114)\\nAddition(3)\\nMT Hallucination(2)\\nOmission(57)\\nUntranslated(0)\\nWrong Name Entity & Term(157)\\nGrammar(30)\\nPunctuation(9)\\nSpelling(3)\\nRegister(8)\\nInconsistent Style(5)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(b) Technology domain.\\nMistranslation(21)\\nAddition(0)\\nMT Hallucination(3)\\nOmission(9)\\nUntranslated(0)\\nWrong Name Entity & Term(49)\\nGrammar(0)\\nPunctuation(2)\\nSpelling(1)\\nRegister(0)\\nInconsistent Style(0)'),\n",
       " Document(page_content='MT Hallucination(3)\\nOmission(9)\\nUntranslated(0)\\nWrong Name Entity & Term(49)\\nGrammar(0)\\nPunctuation(2)\\nSpelling(1)\\nRegister(0)\\nInconsistent Style(0)\\nNon-translation(0)\\nUnnatural Flow(0)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nseamless\\ngpt4\\njunior\\nmedium\\nsenior\\n(c) Biomedical domain.\\nFigure 4: Error category results for different domains in Chinese-to-English. We only include ‘Major’\\nerrors here to highlight the most severe problems. Higher values indicate more errors and the number in\\nthe bracket is the maximum number of that error.\\nSource\\n巨人网络有限公司\\nGPT-4\\nGiant Network Group Inc.\\nHuman\\nGiant Interactive Group Inc.\\nTable 5: Named Entity cases.\\nsame level. Despite slightly more Wrong Name\\nEntity & Terms errors made, GPT-4 performs better\\nthan junior and medium-level translators in other\\naspects.\\nDiscussion'),\n",
       " Document(page_content='Entity & Terms errors made, GPT-4 performs better\\nthan junior and medium-level translators in other\\naspects.\\nDiscussion\\nFor specific domains like technol-\\nogy, we show that GPT-4 is comparable with ju-\\nnior/medium translators. We still notice a similar\\nimbalance issue as in the multilingual setting, but\\nGPT-4’s performance is not as sensitive as in the\\nchange of language.\\n5.4\\nCase Study\\nWe also qualitatively understand the difference be-\\ntween the translations given by GPT-4 and human\\ntranslators.\\nLiteral Translations\\nAmong the error cases, the\\ntypical one is literal translations. Specifically, we\\nfind that GPT-4 sometimes translates with semanti-\\ncally correct, but in-native and literal translations.\\nThis is problematic with named entities, especially'),\n",
       " Document(page_content='cally correct, but in-native and literal translations.\\nThis is problematic with named entities, especially\\nthose occurring less frequently. As shown in Table\\n5, when not knowing the correct translation of ‘巨\\n人网络有限公司’, GPT-4 translates the term word\\nby word. However, the issue of name entities oc-\\ncurs less for human translators, partially because\\nthey would google it to find the correct translation.\\nSource\\nIt’s just a white screen or it times out load-\\ning it, or the page becomes unresponsive!\\nGPT-4\\n它只是一个白屏，要么是加载时超时，\\n要么页面变得无响应了！\\nHuman\\n页面要么显示空白，要么加载超时或是\\n无响应。\\nTable 6: Unnatural-Flow cases. Red represents the\\nliteral translation and green is more natural and\\nnative in Chinese.\\nThus, this issue might be resolved by incorporating\\nweb-search into agent-like translation (Feng et al.,'),\n",
       " Document(page_content='native in Chinese.\\nThus, this issue might be resolved by incorporating\\nweb-search into agent-like translation (Feng et al.,\\n2024; Wu et al., 2024c).\\nExcept for named entities, we notice that the\\nliteral translation causes Unnatural Flows.\\nAs\\nshown in Table 6, when translating ‘It’s just a white\\nscreen’, GPT-4 translates the phrase to ‘它(it)只\\n是(is just)一个(a)白屏(white screen)’, but human\\ntranslator translates this phrase to ‘‘页面显示空\\n白(The page display is white)’’, which represents\\na preciser meaning and follows local conventions.\\nHuman Imagination\\nWe find human translators\\nalso have drawbacks compared to the GPT-4 trans-\\nlator. When the source sentence contains insuffi-\\ncient information to translate, human translators\\ntend to fill the gap by imagination or overthinking.'),\n",
       " Document(page_content='cient information to translate, human translators\\ntend to fill the gap by imagination or overthinking.\\nAn example is given in Table 7. The translator\\nwrongly understands the phrase ‘entering his 2nd\\nyear’ as Daley is a two-year-old baby, but the sen-\\ntence describes a 2nd-year player for sports. This\\nmay be due to daily language habits, misunder-'),\n",
       " Document(page_content='Source\\nHe\\nhas\\nhealth\\nconcerns\\natm\\nbut\\nwe also have Daley entering his 2nd year\\nand is a decent safety net.\\nGPT-4\\n他目前有健康问题，但我们还有戴利进\\n入他的第二年，他是一个不错的安全保\\n障。\\nHuman\\n他目前有健康问题。不过，戴利两岁\\n了，是个不错的备选人。\\nTable 7: Human imagination cases. Red denotes the\\nimagined part.\\nstanding, or not paying attention, and could be\\nrelated to the hallucination (Zhang et al., 2023) of\\nLLMs. GPT-4’s literal translation helps in this, as\\nit keeps faithful to the source sentence. This also\\naligns with our findings in Section 5.1 that GPT-4\\nhas fewer Additions or Omissions.\\n6\\nConclusion\\nIn this study, we comprehensively evaluated the\\ntranslation quality of GPT-4 against human trans-\\nlators of varying expertise levels across multiple\\nlanguage pairs and domains. Our findings showed'),\n",
       " Document(page_content='translation quality of GPT-4 against human trans-\\nlators of varying expertise levels across multiple\\nlanguage pairs and domains. Our findings showed\\nthat GPT-4 performs comparably to junior transla-\\ntors in terms of total errors made but lags behind\\nmedium and senior translators. We also notice that\\nGPT-4’s translation capability gradually weakens\\nfrom resource-rich to resource-poor language pairs.\\nQualitative analysis revealed that GPT-4 tends to\\nproduce more literal translations compared to hu-\\nman translators but suffers less from imagined in-\\nformation.\\nThe results of this study demonstrate that GPT-4\\nhas made significant strides in approaching human-\\nlevel translation quality, as well as highlighting the\\nnuanced difference between them. This suggests'),\n",
       " Document(page_content='level translation quality, as well as highlighting the\\nnuanced difference between them. This suggests\\npromising opportunities for collaboration and en-\\nhancement of translation workflows. As research\\ncontinues to advance, we anticipate that LLMs will\\nbecome increasingly valuable tools in the trans-\\nlation industry, working alongside human transla-\\ntors to improve productivity, efficiency, and overall\\ntranslation quality.\\n7\\nLimitations\\nOur work is limited in the following aspects: (1)\\nWe benchmark GPT-4 for translation tasks, as it is\\na representative large language model and shows\\nstate-of-the-art performance for many text-based\\ntasks. However, our evaluations can be extended\\nto other LLMs such as Claude-3. (2) Our eval-\\nuation covers three languages and six directions'),\n",
       " Document(page_content='tasks. However, our evaluations can be extended\\nto other LLMs such as Claude-3. (2) Our eval-\\nuation covers three languages and six directions\\nfrom resource-rich to resource-poor. However, for\\nother languages, there might be linguistic-specific\\nphenomena that are not covered in this paper.\\nReferences\\nGuangsheng Bao, Yanbin Zhao, Zhiyang Teng,\\nLinyi Yang, and Yue Zhang. 2023.\\nFast-\\ndetectgpt:\\nEfficient zero-shot detection of\\nmachine-generated text via conditional probabil-\\nity curvature. arXiv preprint arXiv:2310.05130.\\nLjubisa Bojic, Predrag Kovacevic, and Milan\\nCabarkapa. 2023.\\nGpt-4 surpassing human\\nperformance in linguistic pragmatics.\\narXiv\\npreprint arXiv:2312.09545.\\nJacob Cohen. 1960. A coefficient of agreement for\\nnominal scales. Educational and psychological'),\n",
       " Document(page_content='arXiv\\npreprint arXiv:2312.09545.\\nJacob Cohen. 1960. A coefficient of agreement for\\nnominal scales. Educational and psychological\\nmeasurement, 20(1):37–46.\\nSeamless Communication, Loïc Barrault, Yu-An\\nChung, Mariano Cora Meglioli, David Dale,\\nNing Dong, Paul-Ambroise Duquenne, Hady\\nElsahar, Hongyu Gong, Kevin Heffernan, John\\nHoffman, Christopher Klaiber, Pengwei Li,\\nDaniel Licht, Jean Maillard, Alice Rakotoari-\\nson, Kaushik Ram Sadagopan, Guillaume Wen-\\nzek, Ethan Ye, Bapi Akula, Peng-Jen Chen,\\nNaji El Hachem, Brian Ellis, Gabriel Mejia\\nGonzalez, Justin Haaheim, Prangthip Hansanti,\\nRuss Howes, Bernie Huang, Min-Jae Hwang,\\nHirofumi Inaguma, Somya Jain, Elahe Kalbassi,\\nAmanda Kallet, Ilia Kulikov, Janice Lam, Daniel\\nLi, Xutai Ma, Ruslan Mavlyutov, Benjamin Pelo-'),\n",
       " Document(page_content='Hirofumi Inaguma, Somya Jain, Elahe Kalbassi,\\nAmanda Kallet, Ilia Kulikov, Janice Lam, Daniel\\nLi, Xutai Ma, Ruslan Mavlyutov, Benjamin Pelo-\\nquin, Mohamed Ramadan, Abinesh Ramakrish-\\nnan, Anna Sun, Kevin Tran, Tuan Tran, Igor\\nTufanov, Vish Vogeti, Carleigh Wood, Yilin\\nYang, Bokai Yu, Pierre Andrews, Can Balioglu,\\nMarta R. Costa-jussà, Onur Celebi, Maha El-\\nbayad, Cynthia Gao, Francisco Guzmán, Justine\\nKao, Ann Lee, Alexandre Mourachko, Juan Pino,\\nSravya Popuri, Christophe Ropers, Safiyyah\\nSaleem, Holger Schwenk, Paden Tomasello,\\nChanghan Wang, Jeff Wang, and Skyler Wang.\\n2023. Seamlessm4t: Massively multilingual &\\nmultimodal machine translation.\\nMaxim Enis and Mark Hopkins. 2024.\\nFrom\\nllm to nmt:\\nAdvancing low-resource ma-'),\n",
       " Document(page_content='chine translation with claude. arXiv preprint\\narXiv:2404.13813.\\nAkhbardeh Farhad,\\nArkhangorodsky Arkady,\\nBiesialska Magdalena, Bojar Ondˇrej, Chatterjee\\nRajen, Chaudhary Vishrav, Marta R Costa-jussa,\\nEspaña-Bonet Cristina, Fan Angela, Federmann\\nChristian, et al. 2021. Findings of the 2021 con-\\nference on machine translation (wmt21). In Pro-\\nceedings of the Sixth Conference on Machine\\nTranslation, pages 1–88. Association for Com-\\nputational Linguistics.\\nZhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu\\nLiao, Wenqiang Liu, Jun Lang, Yang Feng, Jian\\nWu, and Zuozhu Liu. 2024. Tear: Improving\\nllm-based machine translation with systematic\\nself-refinement.\\nLukas Fischer and Samuel L¨\"aubli. 2020. What’s\\nthe difference between professional human and\\nmachine translation?\\na blind multi-language'),\n",
       " Document(page_content='Lukas Fischer and Samuel L¨\"aubli. 2020. What’s\\nthe difference between professional human and\\nmachine translation?\\na blind multi-language\\nstudy on domain-specific MT. In Proceedings\\nof the 22nd Annual Conference of the European\\nAssociation for Machine Translation, pages 215–\\n224, Lisboa, Portugal. European Association for\\nMachine Translation.\\nMarkus Freitag, George Foster, David Grang-\\nier, Viresh Ratnakar, Qijun Tan, and Wolfgang\\nMacherey. 2021. Experts, errors, and context:\\nA large-scale study of human evaluation for ma-\\nchine translation. Transactions of the Associ-\\nation for Computational Linguistics, 9:1460–\\n1474.\\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleft-\\nherios Avramidis, Ricardo Rei, Brian Thompson,\\nTom Kocmi, Frederic Blain, Daniel Deutsch,'),\n",
       " Document(page_content='1474.\\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleft-\\nherios Avramidis, Ricardo Rei, Brian Thompson,\\nTom Kocmi, Frederic Blain, Daniel Deutsch,\\nCraig Stewart, Chrysoula Zerva, Sheila Castilho,\\nAlon Lavie, and George Foster. 2023. Results\\nof WMT23 metrics shared task: Metrics might\\nbe guilty but references are not innocent. In Pro-\\nceedings of the Eighth Conference on Machine\\nTranslation, pages 578–628, Singapore. Associ-\\nation for Computational Linguistics.\\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-\\nkiu Lo, Craig Stewart, Eleftherios Avramidis,\\nTom Kocmi, George Foster, Alon Lavie, and\\nAndré F. T. Martins. 2022. Results of WMT22\\nmetrics shared task: Stop using BLEU – neu-\\nral metrics are better and more robust. In Pro-\\nceedings of the Seventh Conference on Machine'),\n",
       " Document(page_content='metrics shared task: Stop using BLEU – neu-\\nral metrics are better and more robust. In Pro-\\nceedings of the Seventh Conference on Machine\\nTranslation (WMT), pages 46–68, Abu Dhabi,\\nUnited Arab Emirates (Hybrid). Association for\\nComputational Linguistics.\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett.\\n2022. News summarization and evaluation in the\\nera of gpt-3. arXiv preprint arXiv:2209.12356.\\nYvette Graham, Timothy Baldwin, Alistair Mof-\\nfat, and Justin Zobel. 2013. Continuous mea-\\nsurement scales in human evaluation of machine\\ntranslation. In Proceedings of the 7th Linguis-\\ntic Annotation Workshop and Interoperability\\nwith Discourse, pages 33–41, Sofia, Bulgaria.\\nAssociation for Computational Linguistics.\\nYvette Graham, Christian Federmann, Maria Es-'),\n",
       " Document(page_content='with Discourse, pages 33–41, Sofia, Bulgaria.\\nAssociation for Computational Linguistics.\\nYvette Graham, Christian Federmann, Maria Es-\\nkevich, and Barry Haddow. 2020. Assessing\\nhuman-parity in machine translation on the seg-\\nment level. In Findings of the Association for\\nComputational Linguistics: EMNLP 2020, pages\\n4199–4207, Online. Association for Computa-\\ntional Linguistics.\\nTianyu Han, Lisa C Adams, Keno Bressem, Fe-\\nlix Busch, Luisa Huck, Sven Nebelung, and\\nDaniel Truhn. 2023. Comparative analysis of\\ngpt-4vision, gpt-4 and open source llms in clin-\\nical diagnostic accuracy: A benchmark against\\nhuman expertise. medRxiv, pages 2023–11.\\nHany Hassan, Anthony Aue, Chang Chen, Vishal\\nChowdhary, Jonathan Clark, Christian Fed-\\nermann, Xuedong Huang, Marcin Junczys-'),\n",
       " Document(page_content='Hany Hassan, Anthony Aue, Chang Chen, Vishal\\nChowdhary, Jonathan Clark, Christian Fed-\\nermann, Xuedong Huang, Marcin Junczys-\\nDowmunt, William Lewis, Mu Li, et al. 2018.\\nAchieving human parity on automatic chinese\\nto english news translation.\\narXiv preprint\\narXiv:1803.05567.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\nZou, Mantas Mazeika, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring massive multitask\\nlanguage understanding.\\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\\nVikas Raunak, Mohamed Gabr, Hitokazu Mat-\\nsushita, Young Jin Kim, Mohamed Afify, and\\nHany Hassan Awadalla. 2023. How good are gpt\\nmodels at machine translation? a comprehensive\\nevaluation. arXiv preprint arXiv:2302.09210.\\nHui Huang, Shuangzhi Wu, Xinnian Liang, Bing\\nWang, Yanrui Shi, Peihao Wu, Muyun Yang, and'),\n",
       " Document(page_content='evaluation. arXiv preprint arXiv:2302.09210.\\nHui Huang, Shuangzhi Wu, Xinnian Liang, Bing\\nWang, Yanrui Shi, Peihao Wu, Muyun Yang, and\\nTiejun Zhao. 2023. Towards making the most of'),\n",
       " Document(page_content='llm for translation quality estimation. In CCF\\nInternational Conference on Natural Language\\nProcessing and Chinese Computing, pages 375–\\n386. Springer.\\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang,\\nXing Wang, Shuming Shi, and Zhaopeng Tu.\\n2023a. Is chatgpt a good translator? yes with\\ngpt-4 as the engine.\\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang,\\nXing Wang, Shuming Shi, and Zhaopeng Tu.\\n2023b. Is chatgpt a good translator? yes with\\ngpt-4 as the engine.\\nAhrii Kim, Yunju Bak, Jimin Sun, Sungwon Lyu,\\nand Changmin Lee. 2023.\\nThe suboptimal\\nwmt test sets and its impact on human parity.\\nPreprints.\\nFilip Klubiˇcka, Antonio Toral, and Víctor M\\nSánchez-Cartagena. 2018.\\nQuantitative fine-\\ngrained human evaluation of machine transla-\\ntion systems: a case study on english to croatian.'),\n",
       " Document(page_content='Sánchez-Cartagena. 2018.\\nQuantitative fine-\\ngrained human evaluation of machine transla-\\ntion systems: a case study on english to croatian.\\nMachine Translation, 32(3):195–215.\\nTom Kocmi, Eleftherios Avramidis, Rachel Baw-\\nden, Ondˇrej Bojar, Anton Dvorkovich, Chris-\\ntian Federmann, Mark Fishel, Markus Freitag,\\nThamme Gowda, Roman Grundkiewicz, et al.\\n2023. Findings of the 2023 conference on ma-\\nchine translation (wmt23): Llms are here but\\nnot quite there yet. In Proceedings of the Eighth\\nConference on Machine Translation, pages 1–42.\\nTom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton\\nDvorkovich, Christian Federmann, Mark Fishel,\\nThamme Gowda, Yvette Graham, Roman Grund-\\nkiewicz, Barry Haddow, et al. 2022. Findings\\nof the 2022 conference on machine translation'),\n",
       " Document(page_content='Thamme Gowda, Yvette Graham, Roman Grund-\\nkiewicz, Barry Haddow, et al. 2022. Findings\\nof the 2022 conference on machine translation\\n(wmt22). In Proceedings of the Seventh Con-\\nference on Machine Translation (WMT), pages\\n1–45.\\nKlaus Krippendorff. 1980. Validity in content anal-\\nysis. Computerstrategien für die Kommunika-\\ntionsanalyse, 69:45p.\\nYafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue\\nWang, Linyi Yang, Shuming Shi, and Yue Zhang.\\n2023. Deepfake text detection in the wild. arXiv\\npreprint arXiv:2305.13242.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\\nJiang, Hiroaki Hayashi, and Graham Neubig.\\n2023a. Pre-train, prompt, and predict: A sys-\\ntematic survey of prompting methods in natural\\nlanguage processing. ACM Computing Surveys,\\n55(9):1–35.\\nZhengliang Liu, Tianyang Zhong, Yiwei Li, Yu-'),\n",
       " Document(page_content='tematic survey of prompting methods in natural\\nlanguage processing. ACM Computing Surveys,\\n55(9):1–35.\\nZhengliang Liu, Tianyang Zhong, Yiwei Li, Yu-\\ntong Zhang, Yi Pan, Zihao Zhao, Peixin Dong,\\nChao Cao, Yuxiao Liu, Peng Shu, et al. 2023b.\\nEvaluating large language models for radiology\\nnatural language processing.\\narXiv preprint\\narXiv:2307.13693.\\nArle Lommel, Maja Popovic, and Aljoscha Bur-\\nchardt. 2014. Assessing inter-annotator agree-\\nment for translation error annotation. In MTE:\\nWorkshop on Automatic and Manual Metrics for\\nOperational Translation Evaluation, pages 31–\\n37. Language Resources and Evaluation Confer-\\nence Reykjavik.\\nLaurence T Maloney, Maria F Dal Martello, Vi-\\nvian Fei, and Valerie Ma. 2024.\\nA compar-\\nison of human and gpt-4 use of probabilistic'),\n",
       " Document(page_content='ence Reykjavik.\\nLaurence T Maloney, Maria F Dal Martello, Vi-\\nvian Fei, and Valerie Ma. 2024.\\nA compar-\\nison of human and gpt-4 use of probabilistic\\nphrases in a coordination game. Scientific re-\\nports, 14(1):6835.\\nHiroki Nakayama, Takahiro Kubo, Junya Ka-\\nmura, Yasufumi Taniguchi, and Xu Liang.\\n2018.\\ndoccano:\\nText\\nannotation\\ntool\\nfor\\nhuman.\\nSoftware\\navailable\\nfrom\\nhttps://github.com/doccano/doccano.\\nHa Nguyen and Vicki Allan. 2024. Using gpt-4\\nto provide tiered, formative code feedback. In\\nProceedings of the 55th ACM Technical Sympo-\\nsium on Computer Science Education V. 1, pages\\n958–964.\\nKeqin Peng, Liang Ding, Qihuang Zhong, Li Shen,\\nXuebo Liu, Min Zhang, Yuanxin Ouyang, and\\nDacheng Tao. 2023. Towards making the most\\nof chatgpt for machine translation. In Findings'),\n",
       " Document(page_content='Xuebo Liu, Min Zhang, Yuanxin Ouyang, and\\nDacheng Tao. 2023. Towards making the most\\nof chatgpt for machine translation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2023, pages 5622–5633.\\nThierry Poibeau. 2022. On\" human parity\" and\" su-\\nper human performance\" in machine translation\\nevaluation. In Language Resource and Evalua-\\ntion Conference.\\nRicardo Rei, Craig Stewart, Ana C Farinha, and\\nAlon Lavie. 2020a. COMET: A neural frame-\\nwork for MT evaluation. In Proceedings of the'),\n",
       " Document(page_content='2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 2685–\\n2702, Online. Association for Computational\\nLinguistics.\\nRicardo Rei, Craig Stewart, Ana C Farinha, and\\nAlon Lavie. 2020b. Comet: A neural frame-\\nwork for mt evaluation. In Proceedings of the\\n2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 2685–\\n2702.\\nSai Cheong Siu. 2023. Chatgpt and gpt-4 for pro-\\nfessional translators: Exploring the potential of\\nlarge language models in translation. Available\\nat SSRN 4448091.\\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy\\nWay. 2018. Attaining the unattainable? reassess-\\ning claims of human parity in neural machine\\ntranslation. In Proceedings of the Third Confer-\\nence on Machine Translation: Research Papers,'),\n",
       " Document(page_content='ing claims of human parity in neural machine\\ntranslation. In Proceedings of the Third Confer-\\nence on Machine Translation: Research Papers,\\npages 113–123, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui\\nZhang, Dian Yu, Shuming Shi, and Zhaopeng\\nTu. 2023a. Document-level machine translation\\nwith large language models.\\nWenxuan Wang, Wenxiang Jiao, Jingyuan Huang,\\nRuyi Dai, Jen-tse Huang, Zhaopeng Tu, and\\nMichael R Lyu. 2023b. Not all countries cel-\\nebrate thanksgiving: On the cultural dominance\\nin large language models. CoRR.\\nMinghao Wu, Thuy-Trang Vu, Lizhen Qu, George\\nFoster, and Gholamreza Haffari. 2024a. Adapt-\\ning large language models for document-\\nlevel machine translation.\\narXiv preprint\\narXiv:2401.06468.'),\n",
       " Document(page_content='Foster, and Gholamreza Haffari. 2024a. Adapt-\\ning large language models for document-\\nlevel machine translation.\\narXiv preprint\\narXiv:2401.06468.\\nMinghao Wu, Yulin Yuan, Gholamreza Haffari,\\nand Longyue Wang. 2024b. (perhaps) beyond\\nhuman translation: Harnessing multi-agent col-\\nlaboration for translating ultra-long literary texts.\\narXiv preprint arXiv:2405.11804.\\nMinghao Wu, Yulin Yuan, Gholamreza Haffari,\\nand Longyue Wang. 2024c. (perhaps) beyond\\nhuman translation: Harnessing multi-agent col-\\nlaboration for translating ultra-long literary texts.\\nHaoran Xu, Young Jin Kim, Amr Sharaf, and\\nHany Hassan Awadalla. 2023. A paradigm shift\\nin machine translation: Boosting translation per-\\nformance of large language models.\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chen-'),\n",
       " Document(page_content='in machine translation: Boosting translation per-\\nformance of large language models.\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chen-\\njie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian\\nYu, Cong Yu, et al. 2020.\\nClue: A chinese\\nlanguage understanding evaluation benchmark.\\narXiv preprint arXiv:2004.05986.\\nFanghua Ye, Mingming Yang, Jianhui Pang,\\nLongyue Wang, Derek F. Wong, Emine Yilmaz,\\nShuming Shi, and Zhaopeng Tu. 2024. Bench-\\nmarking llms via uncertainty quantification.\\nLifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng\\nGao, Fangyuan Zou, Xingyi Cheng, Heng Ji,\\nZhiyuan Liu, and Maosong Sun. 2023. Revisit-\\ning out-of-distribution robustness in nlp: Bench-\\nmark, analysis, and llms evaluations.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao\\nLiu, Tingchen Fu, Xinting Huang, Enbo Zhao,'),\n",
       " Document(page_content='mark, analysis, and llms evaluations.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao\\nLiu, Tingchen Fu, Xinting Huang, Enbo Zhao,\\nYu Zhang, Yulong Chen, et al. 2023. Siren’s\\nsong in the ai ocean: a survey on hallucina-\\ntion in large language models. arXiv preprint\\narXiv:2309.01219.\\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein,\\nand Sameer Singh. 2021. Calibrate before use:\\nImproving few-shot performance of language\\nmodels. In International conference on machine\\nlearning, pages 12697–12706. PMLR.\\nJie Zhu, Junhui Li, Yalong Wen, and Lifan Guo.\\n2024.\\nBenchmarking large language models\\non cflue–a chinese financial language under-\\nstanding evaluation dataset.\\narXiv preprint\\narXiv:2405.10542.\\nA\\nExpertise of Human Annotators\\nTo categorize translators into junior, medium, or'),\n",
       " Document(page_content='standing evaluation dataset.\\narXiv preprint\\narXiv:2405.10542.\\nA\\nExpertise of Human Annotators\\nTo categorize translators into junior, medium, or\\nsenior levels, we have established a comprehensive\\nset of criteria that take into account various factors\\nindicative of a translator’s expertise and experience.\\nThese factors include the translator’s educational\\nbackground, particularly the prestige of the insti-\\ntution from which they graduated, as well as their\\nlength of service in the translation industry, the\\nduration of their translation career, the number of\\ntranslations completed, and any professional certifi-\\ncations they have obtained. To ensure the ongoing\\ncompetence of our translators, we conduct quar-\\nterly assessments to evaluate their performance.'),\n",
       " Document(page_content='For instance, to be classified as a senior-level trans-\\nlator, an individual must possess a minimum of\\nten years of translation experience, demonstrate\\nexceptional proficiency by achieving a score of\\n99% on our assessments, and hold the distinguished\\nCATTI++ translation certification. By considering\\nthese stringent criteria, we aim to maintain a highly\\nqualified and skilled pool of translators across all\\nlevels of expertise.\\nB\\nAnnotation Requirements\\nB.1\\nError Types\\nOur annotation system is built upon the open-\\nsourced doccano system 3. In Figure 5, we provide\\na screenshot of our annotation system. For each\\nsource sentence, outputs for different systems are\\ngiven and the annotators can select spans of the text\\nand annotate the error type and severity.\\nC'),\n",
       " Document(page_content='source sentence, outputs for different systems are\\ngiven and the annotators can select spans of the text\\nand annotate the error type and severity.\\nC\\nDetailed Explanation and Guidance for\\nEach Error Types\\nOur evaluation protocol largely follows the MQM\\ncriteria released by Unbabel4. We provide a de-\\ntailed annotation manual for annotators, including\\nan explanation for each error type as well as illus-\\ntrative examples for error types. It is included in\\nthe following:\\nC.1\\nAnnotation Requirements\\nThe minimum unit that can be selected and anno-\\ntated is a whole word, a whitespace, a punctuation\\nmark, or an isolated character. In the following\\nexample, the version in French has an extra excla-\\nmation mark, so it’s necessary to annotate it as a\\nPunctuation error:\\n[EN] Thank you very much.'),\n",
       " Document(page_content='example, the version in French has an extra excla-\\nmation mark, so it’s necessary to annotate it as a\\nPunctuation error:\\n[EN] Thank you very much.\\n[FR] Merci beaucoup!\\nWrong\\nselection\\n→Merci\\n[beau-\\ncoup!]PUNCTUATION\\nCorrect\\nselection\\n→Merci\\nbeau-\\ncoup[!]PUNCTUATION\\nIf the issue occurs in a multiword expression,\\nyou will need to select the whole expression; if, for\\nexample, an entire sentence was translated and it\\nshouldn’t have been, you should select the entire\\nsentence.\\n3https://github.com/doccano/doccano\\n4\\nIn the following example, we have an Unnatural\\nFlow error:\\n[EN] Hi, Mary here.\\n[ES] Hola, Mary aquí.\\nWrong\\nselection\\n→\\nHola,\\n[Mary\\naquí.]UNNATURAL FLOW\\nCorrect\\nselection\\n→\\nHola,\\n[Mary\\naquí]UNNATURAL FLOW.\\nC.2\\nError Types\\nAccuracy\\n• Mistranslation'),\n",
       " Document(page_content='Wrong\\nselection\\n→\\nHola,\\n[Mary\\naquí.]UNNATURAL FLOW\\nCorrect\\nselection\\n→\\nHola,\\n[Mary\\naquí]UNNATURAL FLOW.\\nC.2\\nError Types\\nAccuracy\\n• Mistranslation\\n– Description: Translation does not accu-\\nrately represent the source.\\n– Example:\\n[EN] It has to be done by the book.\\n[FR] Il doit être fait [par le\\nlivre]MISTRANSLATION\\n[Reason] The word-for-word trans-\\nlation into French doesn’t work.\\n• Addition\\n– Description: Information not present in\\nthe source.\\n– Example:\\n[EN] That way you can be sure that\\nyou were the one who made the\\nchanges.\\n[ES] Así puedes estar seguro de que\\nfuiste tú quien hizo [todos ADDI-\\ntIoN los cambios.\\n[Reason] [Todos] (meaning ’all’ in\\nSpanish) is not present in the source\\nand it is incorrectly added in the\\ntarget text.\\n• MT Hallucination'),\n",
       " Document(page_content='[Reason] [Todos] (meaning ’all’ in\\nSpanish) is not present in the source\\nand it is incorrectly added in the\\ntarget text.\\n• MT Hallucination\\n– Description: information that has noth-\\ning related to source; or gibberish; or\\nrepeats\\n– Example:\\n[EN] You can send us a follow-up\\nemail at this address [EMAIL].\\n[ES] [Hágame saber si tiene al-\\nguna otra pregunta]MT HALLUCI-\\nNATION.]\\n[Reason]: The Spanish translation'),\n",
       " Document(page_content='Figure 5: A screenshot of the Doccano annotation system we use.\\nreads please let me know if you\\nhave any other questions and it’s\\ngrammatically correct and fluent,\\nbut it has no relation at all with the\\nsource.]\\n• Omission\\n– Description: Missing content from the\\nsource.\\n– Example:\\n[EN] We do not have much informa-\\ntion on this.\\n[FR]\\nNous\\nne\\ndisposons\\npas\\n[]\\nOMISSION\\nbeaucoup\\nd’informations à ce sujet.\\n[Reason]:\\nThe French sentence\\nrequires the preposition [de] (dis-\\nposer de).\\n• Untranslated\\n– Description: Not translated.\\n– Example:\\n[EN] How To Make Pizza Dough\\n[FR] Comment faire de [Pizza\\nDough|UNTRANSLATED\\n[Reason]: [Pizza Dough] is not a\\nnamed entity and is untranslated in\\nthe French version.\\n• Wrong Name Entity & Term\\n– Description: Wrong usage of NE and\\nTerminology.\\n– Example:'),\n",
       " Document(page_content='named entity and is untranslated in\\nthe French version.\\n• Wrong Name Entity & Term\\n– Description: Wrong usage of NE and\\nTerminology.\\n– Example:\\n[EN] Dear Wiley,\\n[IT]\\nGentile\\n[Wilar\\nWRONG\\nNAMED ENTITY,\\n[Reason]: The name in the Italian\\nversion doesn’t match the original.\\nFluency\\n• Grammar\\n– Description: Problems with grammar of\\ntarget language.\\n– Example:\\n[EN] I understand that you want to\\ncheck in online.\\n[CS]\\nchàpu,\\nze\\nse\\nchcete\\n[odbavení]gRAMMaR online.\\n[Reason]: Wrong part of speech\\nmakes the sentence ungrammatical\\nin Czech.\\n• Punctuation\\n– Description: incorrect punctuation (for\\nlocale or style).\\n– Example:'),\n",
       " Document(page_content='[EN] Original copy of the Proof of\\nPurchase or Invoice (not a screen-\\nshot):\\n[PT] C’opia original do com-\\nprovante\\nde\\ncompra\\nou\\nnota\\nfiscal\\n(não\\numa\\ncaptura\\nde\\ntela)[.]PUNCTUATION\\n[Reason]: There’s a period instead\\nof a colon in the Brazilian Por-\\ntuguese version of this sentence.\\n• Spelling\\n– Description: incorrect spelling or capital-\\nization.\\n– Example:\\n[EN] This sort of damage is not cov-\\nered under the warranty, but we will\\nseek assistance from a higher sup-\\nport and see what we can do regard-\\ning this issue.\\n[IT] Questo tipo di danno non è cop-\\nerto dalla garanzia, ma chiederò\\ncomunque aiuto ai responsabili\\ndell’assistenza per capire che cosa\\n[Zi]SPELLING può fare per quanto\\nriguarda questo problema.\\n[Reason]: There’s a typo in the\\nsentence in Italian: the word [zi]'),\n",
       " Document(page_content='[Zi]SPELLING può fare per quanto\\nriguarda questo problema.\\n[Reason]: There’s a typo in the\\nsentence in Italian: the word [zi]\\nshould be [si] instead.\\n• Register\\n– Description: Wrong grammatical regis-\\nter (e.g., inappropriately informal pro-\\nnouns).\\n– Example:\\n[EN] Wishing you a great day\\nahead.\\n[DE]\\nIch\\nwünsche\\n[Ih-\\nnen]REGISTER\\neinen\\nschönen\\nTag.\\n[Reason]: The required register for\\nthe German translation is Informal\\nbut the pronoun [Inhen] is Formal.\\n• Inconsistent Style\\n– Description: internal inconsistency (not\\nrelated to terminology).\\n– Example:\\n[EN] Please click on this link. [...]\\nThis link will expire in 24 hours.\\n[NN]\\nKlikk\\npå\\ndenne\\n[lenken].[...]Denne\\n[linken]INCONSISTENCY\\nut-\\nloper om 24 timer.\\n[Reason]: Both [lenk] and [link]\\nare correct in Norwegian, but in the'),\n",
       " Document(page_content='Klikk\\npå\\ndenne\\n[lenken].[...]Denne\\n[linken]INCONSISTENCY\\nut-\\nloper om 24 timer.\\n[Reason]: Both [lenk] and [link]\\nare correct in Norwegian, but in the\\nsame document, only one should be\\nused. Note: this is a single error,\\nnot two\\n• Unnatural Flow\\n– Description: translations that are too lit-\\neral or sound unnatural.\\n– Example:\\n[EN] Zebras are ideal for animal\\nmatching.\\n[DE]\\n[Zebras\\nsind\\nideal,\\num\\nbestimmte\\nTiere\\nzu\\nfinden]UNNATURAL FLOW.\\n[Reason] The German translation\\nsounds too literal, it reads like a\\ntranslation, using the verb [finden]\\n(finding) as a translation for match-\\ning. The verb matching should be\\ntranslated as [detektieren] (detect)\\nto read as if it was originally written\\nin the target language: [Zebras sind\\nein ideales Beispiel zur Detektion\\nvon Wildtieren.]\\nOther'),\n",
       " Document(page_content='to read as if it was originally written\\nin the target language: [Zebras sind\\nein ideales Beispiel zur Detektion\\nvon Wildtieren.]\\nOther\\n• Non-translation\\nD\\nExtra Details\\nD.1\\nTranslation Prompt in Preliminary Study\\nIn two experiments, the translation prompt we use\\nis as follows:\\n• Please translate the following sentences from\\n<SRC_LANG> to <TGT_LANG>. Ensure\\nline alignment across the document while\\nmaintaining the fluency of overall translation.\\nThe prompt asks GPT4 to maintain the sentence\\nalignment of the given document, so each sentence\\ncan be aligned back to its source sentence while be-\\ning translated at the document level. In practice, we\\nfind most times GPT4 can follow our instructions.'),\n",
       " Document(page_content='Occasionally, it fails to keep the sentence structure\\nof the document and merges some sentences in one\\nrow. In these cases, we manually split the merged\\nsentences.\\nD.2\\nModel and Decoding\\nFor GPT-4, we use greedy search for decoding,\\nto ensure the reproducibility of the results. For\\nSeamlessM4T, we use the 2.3B version of seam-\\nlessM4T_v2_large and adopt beam search with\\nbeam size 5.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = splitter.create_documents([text])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/zjjpj02j0f59s5fjqq0twgp40000gn/T/ipykernel_23285/1836249673.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/Users/arunekambaram/Desktop/RAG-ChatBot/venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Simple hybrid retriever setup\n",
    "from typing import List\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x367e70fa0>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    You are a helpful assistant. Use only the provided context to answer the question.\n",
    "    If the answer is not in the context, say \"The document does not contain that information.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")  # or \"mixtral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='\\n    You are a helpful assistant. Use only the provided context to answer the question.\\n    If the answer is not in the context, say \"The document does not contain that information.\"\\n\\n    Context:\\n    {context}\\n\\n    Question:\\n    {question}\\n    '), llm=Ollama(model='llama3')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x367e70fa0>, search_kwargs={'k': 4}))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is mentioned in the context:\n",
      "\n",
      "\"The average win rate of GPT-4 and human translations to prevent annotators from identifying GPT-4. The average win rate of GPT is 15.5/40 (36.25%).\"\n"
     ]
    }
   ],
   "source": [
    "query = \"The average win rate of GPT\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
